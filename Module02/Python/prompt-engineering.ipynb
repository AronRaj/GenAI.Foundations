{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92869e0",
   "metadata": {},
   "source": [
    "# Module 02: Prompt Engineering for Retail/eCommerce\n",
    "\n",
    "Welcome to Module 02! In this hands-on notebook, you'll learn practical prompt engineering techniques applied to real-world retail/eCommerce scenarios using Walmart data.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Master the principles of effective prompt engineering\n",
    "- Apply zero-shot and few-shot learning techniques\n",
    "- Create reusable prompts with LangChain templates\n",
    "- Understand when to use temperature and top_p parameters\n",
    "- Build practical AI solutions for retail use cases\n",
    "\n",
    "## üìö What You'll Build\n",
    "- Customer review analysis system\n",
    "- Support ticket classifier\n",
    "- Product description generator\n",
    "- Product comparison tool\n",
    "- Automated review response system (bonus)\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e21901",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Initialize Azure OpenAI\n",
    "\n",
    "**üìã Instructions:**\n",
    "1. Run this cell to import all required libraries\n",
    "2. The code will try to load your API key from a `.env` file\n",
    "3. If no `.env` file exists, you'll be prompted to enter your API key\n",
    "4. Ensure `walmart_data.json` exists in `../../data/` directory\n",
    "\n",
    "**üí° What's happening here:**\n",
    "- We're setting up LangChain to work with Azure OpenAI\n",
    "- Creating helper functions to make API calls easier\n",
    "- Loading sample retail data for our exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65df826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import getpass\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, FewShotPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Prompt for API key only if not present in environment variables after loading .env\n",
    "if not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter your Azure OpenAI API key: \")\n",
    "\n",
    "# Initialize Azure OpenAI model\n",
    "model = AzureChatOpenAI(\n",
    "    azure_endpoint=\"https://aoi-ext-eus-aiml-profx-01.openai.azure.com/\",\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    model=\"gpt-4o\",\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    temperature=0.7,\n",
    "    top_p=1.0,\n",
    ")\n",
    "\n",
    "# Load data\n",
    "with open('../../data/walmart_data.json', 'r') as f:\n",
    "    walmart_data = json.load(f)\n",
    "\n",
    "def get_completion(prompt, temperature=0.7):\n",
    "    \"\"\"Get completion from Azure OpenAI using LangChain\"\"\"\n",
    "    temp_model = AzureChatOpenAI(\n",
    "        azure_endpoint=\"https://aoi-ext-eus-aiml-profx-01.openai.azure.com/\",\n",
    "        api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "        model=\"gpt-4o\",\n",
    "        api_version=\"2024-12-01-preview\",\n",
    "        temperature=temperature,\n",
    "        top_p=1.0,\n",
    "    )\n",
    "    response = temp_model.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "def get_completion_with_top_p(prompt, top_p=1.0, temperature=1.0):\n",
    "    \"\"\"Get completion with custom top_p from Azure OpenAI\"\"\"\n",
    "    temp_model = AzureChatOpenAI(\n",
    "        azure_endpoint=\"https://aoi-ext-eus-aiml-profx-01.openai.azure.com/\",\n",
    "        api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "        model=\"gpt-4o\",\n",
    "        api_version=\"2024-12-01-preview\",\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    response = temp_model.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "print(\"‚úÖ Setup complete! Data loaded successfully.\")\n",
    "print(f\"üì¶ Products available: {len(walmart_data['products'])}\")\n",
    "print(f\"‚≠ê Customer reviews: {len(walmart_data['customer_reviews'])}\")\n",
    "print(f\"üé´ Support tickets: {len(walmart_data['customer_support_tickets'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2246444",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Improved Review Analysis Prompt\n",
    "\n",
    "**üéØ Objective:** Learn how to write effective prompts with clear structure and requirements.\n",
    "\n",
    "**üìù Your Task:**\n",
    "Create a well-structured prompt that analyzes customer reviews for actionable insights.\n",
    "\n",
    "**Your prompt should include:**\n",
    "1. **Clear Role Definition** - Who is the AI? (e.g., \"You are a customer insights analyst...\")\n",
    "2. **Specific Task** - What exactly should it do?\n",
    "3. **Structured Output Format** - How should the results be presented?\n",
    "4. **Clear Requirements** - What insights do you want?\n",
    "\n",
    "**üí° Tips:**\n",
    "- Think about what would be most useful for a product team\n",
    "- Use numbered lists or sections for clarity\n",
    "- Ask for actionable recommendations, not just summaries\n",
    "- Consider: sentiment, praise, complaints, recommendations, marketing opportunities\n",
    "\n",
    "**‚úçÔ∏è TODO:** Complete the `improved_prompt` variable below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e12ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reviews for the Samsung TV (Product P001)\n",
    "tv_reviews = [r for r in walmart_data['customer_reviews'] if r['product_id'] == 'P001']\n",
    "\n",
    "# ‚úçÔ∏è TODO: Create your improved prompt below\n",
    "# Replace the placeholder with your well-structured prompt\n",
    "improved_prompt = f\"\"\"\n",
    "YOUR PROMPT HERE\n",
    "\n",
    "Hints to include:\n",
    "- Define the AI's role (customer insights analyst?)\n",
    "- Specify the task (analyze reviews for what product?)\n",
    "- List the reviews: {chr(10).join([f\"{i+1}. Rating: {r['rating']}/5 - {r['review_text']}\" for i, r in enumerate(tv_reviews)])}\n",
    "- Request structured output (sentiment, praise, complaints, recommendations, etc.)\n",
    "\"\"\"\n",
    "\n",
    "# Test your prompt\n",
    "print(\"YOUR PROMPT:\")\n",
    "print(improved_prompt)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"AI RESPONSE:\")\n",
    "print(get_completion(improved_prompt, temperature=0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c849297f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Zero-Shot Support Ticket Classification\n",
    "\n",
    "**üéØ Objective:** Learn zero-shot learning - teaching the AI to perform a task without providing examples.\n",
    "\n",
    "**üìù Your Task:**\n",
    "Create a prompt that classifies support tickets by priority level using only clear definitions.\n",
    "\n",
    "**Your prompt should include:**\n",
    "1. **Role Definition** - Customer support ticket triage specialist\n",
    "2. **Clear Criteria** - Define what makes a ticket CRITICAL, HIGH, MEDIUM, or LOW priority\n",
    "3. **Ticket Information** - Include the ticket details\n",
    "4. **Structured Output** - Priority level, reasoning, and suggested response time\n",
    "\n",
    "**üí° Key Concepts:**\n",
    "- **Zero-shot** means the AI has no examples - only definitions\n",
    "- Use `temperature=0` for consistent classification\n",
    "- Clear criteria make better classifications\n",
    "\n",
    "**Priority Level Examples:**\n",
    "- CRITICAL: Safety issues, data breaches, legal concerns\n",
    "- HIGH: Product defects, significant impact, time-sensitive\n",
    "- MEDIUM: Shipping delays, minor issues, general inquiries\n",
    "- LOW: Questions, feature requests, non-urgent\n",
    "\n",
    "**‚úçÔ∏è TODO:** Complete the `zero_shot_ticket_prompt` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket = walmart_data['customer_support_tickets'][0]\n",
    "\n",
    "# ‚úçÔ∏è TODO: Create your zero-shot classification prompt below\n",
    "zero_shot_ticket_prompt = f\"\"\"\n",
    "YOUR PROMPT HERE\n",
    "\n",
    "Hints to include:\n",
    "- Role: Customer support ticket triage specialist\n",
    "- Define all priority levels (CRITICAL, HIGH, MEDIUM, LOW)\n",
    "- Ticket details:\n",
    "  - Type: {ticket['issue_type']}\n",
    "  - Description: {ticket['description']}\n",
    "  - Status: {ticket['status']}\n",
    "- Request: Priority level, reasoning, and suggested response time\n",
    "\"\"\"\n",
    "\n",
    "print(\"Zero-Shot Ticket Classification:\")\n",
    "print(get_completion(zero_shot_ticket_prompt, temperature=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad43321",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Few-Shot Product Description Writer\n",
    "\n",
    "**üéØ Objective:** Learn few-shot learning - teaching the AI by providing examples of the desired output.\n",
    "\n",
    "**üìù Your Task:**\n",
    "Create a prompt with 2-3 example product descriptions that demonstrate the style you want, then have the AI generate a new description following the same pattern.\n",
    "\n",
    "**Your prompt should include:**\n",
    "1. **Role Definition** - Product description writer for Walmart's website\n",
    "2. **2-3 Quality Examples** - Show the exact style, tone, and format you want\n",
    "3. **Format Placeholders** - Use `{}` for product name, features, and price\n",
    "4. **Consistent Style** - All examples should follow the same pattern\n",
    "\n",
    "**üí° Tips:**\n",
    "- Make examples engaging and customer-focused\n",
    "- Use emojis, excitement, and benefits (not just features)\n",
    "- Keep format consistent across all examples\n",
    "- Use `temperature=0.7` for creative but structured output\n",
    "\n",
    "**Example Style Ideas:**\n",
    "- Start with an exciting hook\n",
    "- Focus on benefits and lifestyle\n",
    "- Include emojis for visual appeal\n",
    "- End with a call-to-action or value statement\n",
    "\n",
    "**‚úçÔ∏è TODO:** Complete the `few_shot_description_prompt` with your examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a107ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úçÔ∏è TODO: Create your few-shot prompt with 2-3 examples\n",
    "few_shot_description_prompt = \"\"\"\n",
    "YOUR PROMPT HERE\n",
    "\n",
    "Structure:\n",
    "1. Role: You are writing engaging product descriptions for Walmart's website\n",
    "2. Examples (provide 2-3):\n",
    "   \n",
    "   Example 1:\n",
    "   Product: [Product Name]\n",
    "   Description: [Your engaging description with emojis, benefits, excitement!]\n",
    "   \n",
    "   Example 2:\n",
    "   Product: [Product Name]\n",
    "   Description: [Another description in the same style]\n",
    "   \n",
    "   [Add Example 3 if you want]\n",
    "\n",
    "3. Now write for this product:\n",
    "   Product: {}\n",
    "   Key Features: {}\n",
    "   Price: ${}\n",
    "   Description:\n",
    "\"\"\"\n",
    "\n",
    "# Test with LEGO set (Product #6 in the dataset)\n",
    "test_product = walmart_data['products'][5]\n",
    "prompt = few_shot_description_prompt.format(\n",
    "    test_product['name'],\n",
    "    test_product['description'],\n",
    "    test_product['price']\n",
    ")\n",
    "\n",
    "print(\"Few-Shot Product Description:\")\n",
    "print(get_completion(prompt, temperature=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309f1019",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Product Comparison Prompt Template\n",
    "\n",
    "**üéØ Objective:** Learn to create reusable prompt templates using LangChain's `PromptTemplate`.\n",
    "\n",
    "**üìù Your Task:**\n",
    "Build a reusable template that compares any two products with customizable criteria and target audience.\n",
    "\n",
    "**Your template should include:**\n",
    "1. **Input Variables** - List all variables needed (product names, prices, descriptions, ratings, criteria, audience)\n",
    "2. **Template String** - The prompt structure with `{variable}` placeholders\n",
    "3. **Structured Output** - Head-to-head comparison, recommendations, value analysis\n",
    "\n",
    "**üí° LangChain Template Basics:**\n",
    "```python\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"var1\", \"var2\", ...],\n",
    "    template=\"Your prompt with {var1} and {var2}\"\n",
    ")\n",
    "```\n",
    "\n",
    "**What makes a good product comparison?**\n",
    "- Side-by-side feature comparison\n",
    "- \"Best for\" recommendations\n",
    "- Value analysis (worth the price?)\n",
    "- Clear final recommendation\n",
    "\n",
    "**‚úçÔ∏è TODO:** Complete the `comparison_template` using LangChain's PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d7b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úçÔ∏è TODO: Create your product comparison template\n",
    "comparison_template = PromptTemplate(\n",
    "    input_variables=[\n",
    "        # TODO: List all variables you need\n",
    "        # Hint: You need details for BOTH products, plus criteria and target audience\n",
    "        # Example: \"product1_name\", \"product1_price\", ...\n",
    "    ],\n",
    "    template=\"\"\"\n",
    "    YOUR TEMPLATE HERE\n",
    "    \n",
    "    Hints:\n",
    "    - Role: Product comparison expert helping Walmart customers\n",
    "    - Product A details: {product1_name}, ${product1_price}, {product1_desc}, {product1_rating}/5\n",
    "    - Product B details: {product2_name}, ${product2_price}, {product2_desc}, {product2_rating}/5\n",
    "    - Comparison criteria: {criteria}\n",
    "    - Target audience: {target_audience}\n",
    "    - Output sections: Head-to-Head, Best For, Value Analysis, Final Recommendation\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Test with Samsung TV vs AirPods (interesting cross-category comparison!)\n",
    "product1 = walmart_data['products'][0]  # Samsung TV\n",
    "product2 = walmart_data['products'][7]  # AirPods\n",
    "\n",
    "# ‚úçÔ∏è TODO: Format the template with actual product data\n",
    "prompt = comparison_template.format(\n",
    "    # Add all your variables here matching what you defined above\n",
    "    # Example:\n",
    "    # product1_name=product1['name'],\n",
    "    # product1_price=product1['price'],\n",
    "    # ...\n",
    ")\n",
    "\n",
    "print(\"Product Comparison Report:\")\n",
    "print(get_completion(prompt, temperature=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ff8f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Choosing the Right Parameters\n",
    "\n",
    "**üéØ Objective:** Learn when to use different `temperature` and `top_p` values for different tasks.\n",
    "\n",
    "### üîç Understanding the Parameters:\n",
    "\n",
    "**Temperature (0.0 to 2.0):**\n",
    "- **0.0-0.3**: Deterministic, factual, consistent (data extraction, classification)\n",
    "- **0.4-0.7**: Balanced creativity and consistency (general tasks)\n",
    "- **0.8-1.5+**: Highly creative, diverse, varied (brainstorming, creative writing)\n",
    "\n",
    "**Top_p (0.0 to 1.0):**\n",
    "- **0.1-0.5**: Very focused, limited vocabulary\n",
    "- **0.6-0.9**: Balanced diversity\n",
    "- **0.95-1.0**: Maximum diversity\n",
    "\n",
    "---\n",
    "\n",
    "### Scenario 1: Data Extraction\n",
    "\n",
    "**Task:** Extract the product ID from a support ticket.\n",
    "\n",
    "**ü§î Think about it:**\n",
    "- Do we want creativity or accuracy?\n",
    "- Should the output be the same every time?\n",
    "- What temperature should we use?\n",
    "\n",
    "**‚úçÔ∏è TODO:** Choose appropriate parameters and explain your reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b1226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket = walmart_data['customer_support_tickets'][0]\n",
    "\n",
    "# ‚úçÔ∏è TODO: Create a prompt to extract product ID\n",
    "extraction_prompt = f\"\"\"\n",
    "YOUR PROMPT HERE\n",
    "\n",
    "Task: Extract the product ID from this support ticket\n",
    "Ticket Description: {ticket['description']}\n",
    "\n",
    "Hint: The product ID is mentioned in the ticket. Just extract it cleanly.\n",
    "\"\"\"\n",
    "\n",
    "# ‚úçÔ∏è TODO: Choose appropriate temperature and top_p values\n",
    "# Think: Should this be creative or deterministic? Why?\n",
    "temperature = 0.0  # TODO: Is this correct? Change if needed and explain why below\n",
    "top_p = 1.0        # TODO: Should you adjust this? Explain why below\n",
    "\n",
    "# Your reasoning:\n",
    "reasoning = \"\"\"\n",
    "TODO: Explain your parameter choices here\n",
    "- Why did you choose this temperature?\n",
    "- Is consistency or creativity more important for data extraction?\n",
    "- What happens if temperature is too high?\n",
    "\"\"\"\n",
    "\n",
    "print(\"Scenario 1: Data Extraction\")\n",
    "print(f\"Temperature: {temperature}\")\n",
    "print(f\"Top_p: {top_p}\")\n",
    "print(f\"\\nüß† Your Reasoning:\\n{reasoning}\\n\")\n",
    "\n",
    "# Test multiple times to verify consistency\n",
    "print(\"Testing consistency (should outputs be identical?):\")\n",
    "for i in range(3):\n",
    "    response = get_completion_with_top_p(extraction_prompt, top_p=top_p, temperature=temperature)\n",
    "    print(f\"  Attempt {i+1}: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25340006",
   "metadata": {},
   "source": [
    "### Scenario 2: Creative Product Bundles\n",
    "\n",
    "**Task:** Generate creative marketing ideas for product bundles.\n",
    "\n",
    "**ü§î Think about it:**\n",
    "- Do we want the same idea every time or diverse options?\n",
    "- Should the AI be creative and innovative?\n",
    "- What temperature should we use?\n",
    "\n",
    "**‚úçÔ∏è TODO:** Choose appropriate parameters and explain your reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1223abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = walmart_data['products'][:3]\n",
    "\n",
    "# ‚úçÔ∏è TODO: Create a prompt to generate creative bundle ideas\n",
    "bundle_prompt = f\"\"\"\n",
    "YOUR PROMPT HERE\n",
    "\n",
    "Task: Create 3 creative product bundle ideas combining these products:\n",
    "1. {products[0]['name']} - ${products[0]['price']}\n",
    "2. {products[1]['name']} - ${products[1]['price']}\n",
    "3. {products[2]['name']} - ${products[2]['price']}\n",
    "\n",
    "For each bundle, include:\n",
    "- Catchy, memorable name\n",
    "- Value proposition\n",
    "- Bundle price with discount\n",
    "- Target customer\n",
    "\"\"\"\n",
    "\n",
    "# ‚úçÔ∏è TODO: Choose appropriate temperature and top_p for CREATIVITY\n",
    "# Think: Should this be creative or deterministic? Why?\n",
    "temperature = 0.0  # TODO: Is this good for creative tasks? Change it!\n",
    "top_p = 1.0        # TODO: Should you adjust this?\n",
    "\n",
    "# Your reasoning:\n",
    "reasoning = \"\"\"\n",
    "TODO: Explain your parameter choices here\n",
    "- Why did you choose this temperature for creative brainstorming?\n",
    "- Do you want diverse ideas or consistent outputs?\n",
    "- What's the difference between temperature 0.0 vs 0.9 for this task?\n",
    "\"\"\"\n",
    "\n",
    "print(\"Scenario 2: Creative Bundle Generation\")\n",
    "print(f\"Temperature: {temperature}\")\n",
    "print(f\"Top_p: {top_p}\")\n",
    "print(f\"\\nüß† Your Reasoning:\\n{reasoning}\\n\")\n",
    "\n",
    "# Generate multiple variations to see diversity\n",
    "print(\"Generated Bundle Ideas (observe the variation):\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Variation {i+1}:\")\n",
    "    print('='*80)\n",
    "    response = get_completion_with_top_p(bundle_prompt, top_p=top_p, temperature=temperature)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f73414",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåü Bonus Challenge: Automated Review Response System\n",
    "\n",
    "**üéØ Objective:** Combine everything you've learned into a real-world application!\n",
    "\n",
    "**üìù Your Challenge:**\n",
    "Build a class that automatically generates personalized responses to customer reviews based on their rating and sentiment.\n",
    "\n",
    "**Requirements:**\n",
    "1. Create a `ReviewResponseSystem` class\n",
    "2. Use **few-shot learning** with examples of good responses for different ratings\n",
    "3. Choose appropriate **temperature** (balance consistency with warmth)\n",
    "4. Handle different **rating levels** (1-5 stars) appropriately\n",
    "\n",
    "**üí° Design Considerations:**\n",
    "- **5-star reviews**: Enthusiastic thanks, encourage sharing\n",
    "- **3-star reviews**: Acknowledge feedback, offer improvement\n",
    "- **1-star reviews**: Apologize sincerely, offer immediate resolution\n",
    "\n",
    "**What you'll practice:**\n",
    "- Object-oriented design\n",
    "- Few-shot prompt engineering\n",
    "- Parameter selection\n",
    "- Real-world application thinking\n",
    "\n",
    "**This is optional but highly recommended for deepening your understanding!**\n",
    "\n",
    "**‚úçÔ∏è TODO:** Complete the `ReviewResponseSystem` class below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d04f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úçÔ∏è BONUS: Create an automated review response system\n",
    "\n",
    "class ReviewResponseSystem:\n",
    "    \"\"\"Automated system for responding to customer reviews\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # TODO: Initialize Azure OpenAI client using LangChain\n",
    "        # Hint: Use AzureChatOpenAI with appropriate settings\n",
    "        self.client = None  # Replace with your AzureChatOpenAI(...) configuration\n",
    "        \n",
    "        # TODO: Create few-shot examples for different rating levels\n",
    "        # Create examples for 5-star, 3-star, and 1-star reviews\n",
    "        self.examples = [\n",
    "            {\n",
    "                \"rating\": \"5\",\n",
    "                \"review\": \"YOUR EXAMPLE 5-STAR REVIEW\",\n",
    "                \"response\": \"YOUR EXAMPLE RESPONSE TO 5-STAR REVIEW\"\n",
    "            },\n",
    "            # Add more examples for different rating levels\n",
    "        ]\n",
    "    \n",
    "    def generate_response(self, review_text, rating):\n",
    "        \"\"\"Generate appropriate response based on review sentiment\"\"\"\n",
    "        \n",
    "        # TODO: Build few-shot prompt using your examples\n",
    "        # Format your examples into a string showing the pattern\n",
    "        examples_text = \"\"  # Build this from self.examples\n",
    "        \n",
    "        # TODO: Create the prompt\n",
    "        prompt = f\"\"\"\n",
    "        YOUR PROMPT HERE\n",
    "        \n",
    "        Hints:\n",
    "        - Role: Walmart customer service representative\n",
    "        - Show examples: {examples_text}\n",
    "        - Current review to respond to: Rating {rating}/5: {review_text}\n",
    "        - Request: Generate an appropriate response\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Use the client to generate response\n",
    "        # response = self.client.invoke(prompt)\n",
    "        # return response.content\n",
    "        \n",
    "        return \"TODO: Implement response generation\"\n",
    "\n",
    "# ‚úçÔ∏è TODO: Test your system (uncomment when ready)\n",
    "# system = ReviewResponseSystem()\n",
    "# \n",
    "# print(\"ü§ñ Automated Review Response System\\n\")\n",
    "# print(\"=\"*80)\n",
    "# \n",
    "# # Test with first 3 reviews from the dataset\n",
    "# for i, review in enumerate(walmart_data['customer_reviews'][:3], 1):\n",
    "#     print(f\"\\nüìù Review {i}:\")\n",
    "#     print(f\"   Rating: {review['rating']}/5\")\n",
    "#     print(f\"   Review: {review['review_text']}\")\n",
    "#     print(f\"\\nüí¨ Generated Response:\")\n",
    "#     response = system.generate_response(review['review_text'], review['rating'])\n",
    "#     print(f\"   {response}\")\n",
    "#     print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152736b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Summary: Key Takeaways\n",
    "\n",
    "### What You Learned in Module 02:\n",
    "\n",
    "#### 1Ô∏è‚É£ **Prompt Engineering Principles**\n",
    "- ‚úÖ Define clear roles for the AI\n",
    "- ‚úÖ Structure output with numbered lists and sections\n",
    "- ‚úÖ Be specific about requirements\n",
    "- ‚úÖ Request actionable insights, not just summaries\n",
    "\n",
    "#### 2Ô∏è‚É£ **Zero-Shot Learning**\n",
    "- ‚úÖ Teach tasks using only clear definitions and criteria\n",
    "- ‚úÖ No examples needed - just good instructions\n",
    "- ‚úÖ Use `temperature=0` for consistent, factual tasks\n",
    "- ‚úÖ Great for classification, extraction, and structured tasks\n",
    "\n",
    "#### 3Ô∏è‚É£ **Few-Shot Learning**\n",
    "- ‚úÖ Teach by showing 2-5 examples of desired output\n",
    "- ‚úÖ Examples demonstrate style, tone, and format\n",
    "- ‚úÖ More examples = better pattern matching\n",
    "- ‚úÖ Great for creative tasks with specific styles\n",
    "\n",
    "#### 4Ô∏è‚É£ **Prompt Templates with LangChain**\n",
    "- ‚úÖ Create reusable prompts with variables\n",
    "- ‚úÖ Use `PromptTemplate` for consistency\n",
    "- ‚úÖ Easy to maintain and modify\n",
    "- ‚úÖ Perfect for production applications\n",
    "\n",
    "#### 5Ô∏è‚É£ **Temperature & Top_p Selection**\n",
    "| Task Type | Temperature | Top_p | Why? |\n",
    "|-----------|-------------|-------|------|\n",
    "| Data Extraction | 0.0-0.3 | 1.0 | Consistency, accuracy |\n",
    "| Classification | 0.0-0.2 | 1.0 | Deterministic results |\n",
    "| General Writing | 0.5-0.7 | 0.9-1.0 | Balance creativity & structure |\n",
    "| Creative Brainstorming | 0.8-1.2 | 0.95 | Diverse, innovative ideas |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Next Steps:\n",
    "\n",
    "1. **Compare with Solutions**: Check `prompt-engineering-solution.ipynb` to see different approaches\n",
    "2. **Experiment**: Try different temperature values and observe the changes\n",
    "3. **Apply**: Use these techniques in your own projects\n",
    "4. **Practice**: The more prompts you write, the better you'll get!\n",
    "\n",
    "---\n",
    "\n",
    "### üìñ Additional Resources:\n",
    "\n",
    "- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [LangChain Prompts Documentation](https://python.langchain.com/docs/modules/model_io/prompts/)\n",
    "- [Azure OpenAI Best Practices](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering)\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **Congratulations on completing Module 02!** \n",
    "\n",
    "You now have practical skills in prompt engineering that you can apply to real-world AI applications. Keep experimenting and building! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
