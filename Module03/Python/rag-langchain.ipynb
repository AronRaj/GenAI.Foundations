{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff22642",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation) with LangChain\n",
    "\n",
    "This notebook demonstrates how to build a complete RAG pipeline using LangChain and Azure OpenAI. RAG enhances language model responses by retrieving relevant context from a knowledge base before generating answers.\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that combines:\n",
    "- **Retrieval**: Finding relevant information from a knowledge base\n",
    "- **Generation**: Using an LLM to create answers based on retrieved context\n",
    "\n",
    "This approach helps LLMs provide accurate, grounded responses without hallucinating information.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "We'll cover the following steps:\n",
    "1. **Environment Setup**: Load API credentials securely\n",
    "2. **Initialize Embeddings**: Create an embedding model for converting text to vectors\n",
    "3. **Initialize LLM**: Set up the language model for generating responses\n",
    "4. **Create Vector Store**: Initialize an in-memory vector database\n",
    "5. **Load Documents**: Fetch and parse web content\n",
    "6. **Split Documents**: Break large documents into smaller chunks\n",
    "7. **Store Embeddings**: Add document chunks to the vector store\n",
    "8. **Create RAG Chain**: Build a retrieval-augmented generation workflow\n",
    "9. **Test the System**: Query the RAG system and stream results\n",
    "\n",
    "Let's build an intelligent question-answering system!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb26002",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and API Key Management\n",
    "\n",
    "This section handles the secure loading of API credentials.\n",
    "\n",
    "### Your Task:\n",
    "Set up environment variables for Azure OpenAI API access.\n",
    "\n",
    "**Steps:**\n",
    "1. Import required modules:\n",
    "   - `load_dotenv` from `dotenv` (loads environment variables from .env file)\n",
    "   - `getpass` (for secure password input)\n",
    "   - `os` (for environment variable access)\n",
    "2. Call `load_dotenv()` to load environment variables from a `.env` file if present\n",
    "3. Check if `AZURE_OPENAI_API_KEY` exists in environment variables\n",
    "4. If not found, prompt the user securely: `os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter your Azure OpenAI API key: \")`\n",
    "\n",
    "**Security Best Practice:** \n",
    "- Never hardcode API keys in code\n",
    "- Use environment variables or secure prompts\n",
    "- Works both locally (with `.env` file) and in production (with system environment variables)\n",
    "\n",
    "**Expected Output:** No visible output, but the API key will be securely stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bbcecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import load_dotenv from dotenv, getpass, and os\n",
    "\n",
    "\n",
    "# TODO: Load environment variables from .env file\n",
    "\n",
    "\n",
    "# TODO: Check if AZURE_OPENAI_API_KEY exists, if not, prompt for it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a413b79f",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Embedding Model\n",
    "\n",
    "**Embeddings** convert text into numerical vectors that capture semantic meaning.\n",
    "\n",
    "### Your Task:\n",
    "Configure Azure OpenAI embeddings for converting text to vectors.\n",
    "\n",
    "**Steps:**\n",
    "1. Import `AzureOpenAIEmbeddings` from `langchain_openai`\n",
    "2. Create an embeddings instance with these parameters:\n",
    "   - `azure_endpoint=\"https://aoi-ext-eus-aiml-profx-01.openai.azure.com/\"`\n",
    "   - `api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]`\n",
    "   - `model=\"text-embedding-ada-002\"`\n",
    "   - `api_version=\"2024-12-01-preview\"`\n",
    "\n",
    "### Why embeddings matter in RAG:\n",
    "Similar concepts will have similar vector representations, allowing us to find relevant documents by comparing vector similarity rather than exact keyword matches.\n",
    "\n",
    "**Model Details:**\n",
    "- **text-embedding-ada-002**: Azure OpenAI's powerful embedding model\n",
    "- **Output**: 1536-dimensional vectors\n",
    "- **Use Cases**: Semantic search, clustering, similarity comparison\n",
    "\n",
    "**Expected Output:** No output, but the `embeddings` object will be ready to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d73307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import AzureOpenAIEmbeddings from langchain_openai\n",
    "\n",
    "\n",
    "# TODO: Create an AzureOpenAIEmbeddings instance with the required parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42928cc9",
   "metadata": {},
   "source": [
    "## Step 3: Initialize the Language Model (LLM)\n",
    "\n",
    "**Large Language Models (LLMs)** generate human-like text responses based on input prompts.\n",
    "\n",
    "### Your Task:\n",
    "Set up Azure Chat OpenAI for generating answers.\n",
    "\n",
    "**Steps:**\n",
    "1. Import `AzureChatOpenAI` from `langchain_openai`\n",
    "2. Create an LLM instance with these parameters:\n",
    "   - `azure_endpoint=\"https://aoi-ext-eus-aiml-profx-01.openai.azure.com/\"`\n",
    "   - `api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]`\n",
    "   - `model=\"gpt-4o\"`\n",
    "   - `api_version=\"2024-12-01-preview\"`\n",
    "\n",
    "### Role in RAG:\n",
    "The LLM will generate the final answer by:\n",
    "1. Receiving the user's question\n",
    "2. Receiving relevant context retrieved from the vector store\n",
    "3. Synthesizing a response that combines both\n",
    "\n",
    "**Model Details:**\n",
    "- **gpt-4o**: GPT-4 Optimized version\n",
    "- **Capabilities**: Advanced reasoning, following instructions, grounded responses\n",
    "\n",
    "**Expected Output:** No output, but the `llm` object will be ready to generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea64eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import AzureChatOpenAI from langchain_openai\n",
    "\n",
    "\n",
    "# TODO: Create an AzureChatOpenAI instance with the required parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e240a3",
   "metadata": {},
   "source": [
    "## Step 4: Create the Vector Store\n",
    "\n",
    "**Vector Stores** store document embeddings and enable fast similarity searches.\n",
    "\n",
    "### Your Task:\n",
    "Initialize an in-memory vector store for storing document embeddings.\n",
    "\n",
    "**Steps:**\n",
    "1. Import `InMemoryVectorStore` from `langchain_core.vectorstores`\n",
    "2. Create a vector store instance: `vector_store = InMemoryVectorStore(embeddings)`\n",
    "\n",
    "### What is InMemoryVectorStore?\n",
    "- Stores vectors in RAM (not persisted to disk)\n",
    "- Fast for development and small datasets\n",
    "- Automatically uses the embeddings model we configured earlier\n",
    "- Data exists only during the session\n",
    "\n",
    "### How it works:\n",
    "1. Documents are converted to embeddings using our embedding model\n",
    "2. Embeddings are stored along with the original text\n",
    "3. When queried, it finds the most similar vectors using cosine similarity\n",
    "\n",
    "**Production Alternatives:**\n",
    "For production use, consider:\n",
    "- **Chroma**: Local, persistent vector database\n",
    "- **Pinecone**: Managed cloud vector database\n",
    "- **Azure AI Search**: Azure's vector search service\n",
    "- **Weaviate**: Open-source vector database\n",
    "\n",
    "**Expected Output:** No output, but the vector store is ready to store embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d68b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import InMemoryVectorStore from langchain_core.vectorstores\n",
    "\n",
    "\n",
    "# TODO: Create an InMemoryVectorStore instance with the embeddings object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e975ada6",
   "metadata": {},
   "source": [
    "## Step 5: Load Documents from the Web\n",
    "\n",
    "**Document Loading** is the first step in building a knowledge base for RAG.\n",
    "\n",
    "### Your Task:\n",
    "Load content from a blog post about AI agents.\n",
    "\n",
    "**Steps:**\n",
    "1. Import required modules:\n",
    "   - `bs4` (BeautifulSoup for HTML parsing)\n",
    "   - `WebBaseLoader` from `langchain_community.document_loaders`\n",
    "2. Create a BeautifulSoup strainer to filter HTML:\n",
    "   - `bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))`\n",
    "   - This keeps only the main content, removing navigation, ads, etc.\n",
    "3. Create a WebBaseLoader with:\n",
    "   - `web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",)`\n",
    "   - `bs_kwargs={\"parse_only\": bs4_strainer}`\n",
    "4. Load the documents: `docs = loader.load()`\n",
    "5. Verify one document was loaded: `assert len(docs) == 1`\n",
    "6. Print the total characters: `print(f\"Total characters: {len(docs[0].page_content)}\")`\n",
    "\n",
    "### Why filter HTML?\n",
    "- Reduces noise (ads, navigation, comments)\n",
    "- Focuses on the main content\n",
    "- Improves embedding quality and retrieval accuracy\n",
    "\n",
    "**Expected Output:** Message showing the total number of characters loaded (should be several thousand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa028a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import bs4 and WebBaseLoader\n",
    "\n",
    "\n",
    "# TODO: Create a SoupStrainer to filter HTML (keep only post-title, post-header, post-content classes)\n",
    "\n",
    "\n",
    "# TODO: Create a WebBaseLoader with the blog URL and bs_kwargs\n",
    "\n",
    "\n",
    "# TODO: Load the documents\n",
    "\n",
    "\n",
    "# TODO: Assert that exactly one document was loaded\n",
    "\n",
    "\n",
    "# TODO: Print the total number of characters in the loaded document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23a022f",
   "metadata": {},
   "source": [
    "### Preview the Document Content\n",
    "\n",
    "Let's inspect the loaded content to verify it was extracted correctly.\n",
    "\n",
    "### Your Task:\n",
    "Print the first 500 characters of the loaded document.\n",
    "\n",
    "**Steps:**\n",
    "1. Print the first 500 characters: `print(docs[0].page_content[:500])`\n",
    "\n",
    "**Expected Output:** A preview of the blog post content showing the title and beginning of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28639c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print the first 500 characters of the document content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fdb8d3",
   "metadata": {},
   "source": [
    "## Step 6: Split Documents into Chunks\n",
    "\n",
    "**Why split documents?**\n",
    "- LLMs have context window limits (maximum input size)\n",
    "- Smaller chunks create more precise embeddings\n",
    "- Retrieval can target specific relevant sections instead of entire documents\n",
    "\n",
    "### Your Task:\n",
    "Split the large blog post into manageable chunks.\n",
    "\n",
    "**Steps:**\n",
    "1. Import `RecursiveCharacterTextSplitter` from `langchain_text_splitters`\n",
    "2. Create a text splitter with these parameters:\n",
    "   - `chunk_size=1000` (approximately 1000 characters per chunk)\n",
    "   - `chunk_overlap=200` (consecutive chunks share 200 characters)\n",
    "   - `add_start_index=True` (track position in original document)\n",
    "3. Split the documents: `all_splits = text_splitter.split_documents(docs)`\n",
    "4. Print the number of chunks: `print(f\"Split blog post into {len(all_splits)} sub-documents.\")`\n",
    "\n",
    "### The RecursiveCharacterTextSplitter:\n",
    "- Tries to split on natural boundaries (paragraphs, sentences, words)\n",
    "- Falls back to character-level splitting if needed\n",
    "- Preserves semantic coherence within chunks\n",
    "\n",
    "**Expected Output:** Message showing the blog post was split into multiple chunks (typically 40-80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31386df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import RecursiveCharacterTextSplitter from langchain_text_splitters\n",
    "\n",
    "\n",
    "# TODO: Create a text splitter with chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    "\n",
    "\n",
    "# TODO: Split the documents using split_documents()\n",
    "\n",
    "\n",
    "# TODO: Print the number of chunks created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6969a8",
   "metadata": {},
   "source": [
    "## Step 7: Add Documents to Vector Store\n",
    "\n",
    "**Indexing** the documents by converting them to embeddings and storing them.\n",
    "\n",
    "### Your Task:\n",
    "Store all document chunks in the vector store with their embeddings.\n",
    "\n",
    "**Steps:**\n",
    "1. Call `vector_store.add_documents()` with the split documents: `document_ids = vector_store.add_documents(documents=all_splits)`\n",
    "2. Print the first 3 IDs: `print(document_ids[:3])`\n",
    "\n",
    "### What happens during indexing:\n",
    "1. Each document chunk is sent to the embedding model\n",
    "2. The embedding model returns a vector for each chunk\n",
    "3. The vector store saves both the embedding and the original text\n",
    "4. Returns unique IDs for each stored document\n",
    "\n",
    "### The Magic:\n",
    "- Documents with similar semantic meaning will have similar embedding vectors\n",
    "- When we later search with a question, the vector store can find chunks with similar embeddings\n",
    "- This enables **semantic search** (meaning-based) rather than keyword matching\n",
    "\n",
    "**Expected Output:** A list showing the first 3 document IDs (UUIDs)\n",
    "\n",
    "**⚠️ Note:** This may take 30-60 seconds as it generates embeddings for all chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fa9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add all document chunks to the vector store\n",
    "\n",
    "\n",
    "# TODO: Print the first 3 document IDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb7d3d",
   "metadata": {},
   "source": [
    "## Step 8: Load the RAG Prompt Template\n",
    "\n",
    "**Prompt Engineering** is crucial for effective RAG systems.\n",
    "\n",
    "### Your Task:\n",
    "Load a pre-built RAG prompt template from LangChain Hub.\n",
    "\n",
    "**Steps:**\n",
    "1. Import `hub` from `langchain`\n",
    "2. Load the RAG prompt: `prompt = hub.pull(\"rlm/rag-prompt\")`\n",
    "3. Preview the prompt by invoking it with example data:\n",
    "   - `example_messages = prompt.invoke({\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}).to_messages()`\n",
    "4. Assert one message exists: `assert len(example_messages) == 1`\n",
    "5. Print the prompt content: `print(example_messages[0].content)`\n",
    "\n",
    "### What is `hub.pull()`?\n",
    "- Loads a pre-built prompt template from LangChain Hub\n",
    "- **`\"rlm/rag-prompt\"`**: A well-tested RAG prompt template\n",
    "\n",
    "### The RAG Prompt Structure:\n",
    "The template instructs the LLM to:\n",
    "1. Use the provided context to answer the question\n",
    "2. Say \"I don't know\" if the context doesn't contain the answer\n",
    "3. Keep answers concise and grounded in the context\n",
    "\n",
    "### Why use a template?\n",
    "- Consistent response quality\n",
    "- Reduces hallucinations (making up information)\n",
    "- Ensures the LLM leverages the retrieved context effectively\n",
    "\n",
    "**Expected Output:** The formatted prompt template showing how context and questions will be combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fee8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import hub from langchain\n",
    "\n",
    "\n",
    "# TODO: Pull the RAG prompt template from LangChain Hub\n",
    "\n",
    "\n",
    "# TODO: Create example messages by invoking the prompt with placeholder context and question\n",
    "\n",
    "\n",
    "# TODO: Assert that one message was created\n",
    "\n",
    "\n",
    "# TODO: Print the prompt content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439cf11e",
   "metadata": {},
   "source": [
    "## Step 9: Define the State Schema\n",
    "\n",
    "**State Management** for our RAG workflow using TypedDict.\n",
    "\n",
    "### Your Task:\n",
    "Define a state schema that will flow through the RAG pipeline.\n",
    "\n",
    "**Steps:**\n",
    "1. Import required types:\n",
    "   - `Document` from `langchain_core.documents`\n",
    "   - `List` and `TypedDict` from `typing_extensions`\n",
    "2. Create a `State` class that inherits from `TypedDict` with three fields:\n",
    "   - `question`: str (the user's input query)\n",
    "   - `context`: List[Document] (retrieved documents)\n",
    "   - `answer`: str (the LLM's generated response)\n",
    "\n",
    "### What is State?\n",
    "A structured data container that flows through our RAG pipeline, containing all necessary information at each step.\n",
    "\n",
    "### Why define State?\n",
    "- **Type Safety**: Ensures each step receives and returns the correct data types\n",
    "- **Clarity**: Makes the data flow explicit and easier to debug\n",
    "- **LangGraph Integration**: LangGraph uses this schema to manage state between nodes\n",
    "\n",
    "**Expected Output:** No output, but the `State` type is now defined for use in the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e72553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import Document from langchain_core.documents\n",
    "\n",
    "\n",
    "# TODO: Import List and TypedDict from typing_extensions\n",
    "\n",
    "\n",
    "# TODO: Define a State class (TypedDict) with question (str), context (List[Document]), and answer (str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d387b87",
   "metadata": {},
   "source": [
    "## Step 10: Define RAG Workflow Functions\n",
    "\n",
    "**The two core functions of the RAG pipeline:**\n",
    "\n",
    "### Your Task:\n",
    "Create the retrieval and generation functions for the RAG workflow.\n",
    "\n",
    "**Function 1: `retrieve(state: State)`**\n",
    "- Takes the current state containing the question\n",
    "- Uses `vector_store.similarity_search()` to find relevant documents\n",
    "- Returns a dictionary with `context` key containing the retrieved documents\n",
    "\n",
    "**Function 2: `generate(state: State)`**\n",
    "- Takes the state containing question and context\n",
    "- Combines all document content into a single string using `\"\\n\\n\".join()`\n",
    "- Formats the prompt with question and context\n",
    "- Invokes the LLM to generate an answer\n",
    "- Returns a dictionary with `answer` key containing the LLM's response content\n",
    "\n",
    "**Steps:**\n",
    "1. Define `retrieve(state: State)`:\n",
    "   - Get documents: `retrieved_docs = vector_store.similarity_search(state[\"question\"])`\n",
    "   - Return: `{\"context\": retrieved_docs}`\n",
    "   \n",
    "2. Define `generate(state: State)`:\n",
    "   - Join documents: `docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])`\n",
    "   - Format prompt: `messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})`\n",
    "   - Get response: `response = llm.invoke(messages)`\n",
    "   - Return: `{\"answer\": response.content}`\n",
    "\n",
    "### The RAG Flow:\n",
    "**Question → Retrieve Context → Generate Answer**\n",
    "\n",
    "**Expected Output:** No output, but the two functions are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd8b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the retrieve function that takes state and returns context\n",
    "\n",
    "\n",
    "# TODO: Define the generate function that takes state and returns answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a95f01",
   "metadata": {},
   "source": [
    "## Step 11: Build the RAG Graph with LangGraph\n",
    "\n",
    "**LangGraph** creates a stateful, directed workflow for our RAG pipeline.\n",
    "\n",
    "### Your Task:\n",
    "Build and compile the RAG workflow graph.\n",
    "\n",
    "**Steps:**\n",
    "1. Import required classes:\n",
    "   - `START` and `StateGraph` from `langgraph.graph`\n",
    "2. Create a graph builder: `graph_builder = StateGraph(State)`\n",
    "3. Add the workflow sequence: `graph_builder.add_sequence([retrieve, generate])`\n",
    "   - This chains the functions: retrieve → generate\n",
    "4. Add the entry edge: `graph_builder.add_edge(START, \"retrieve\")`\n",
    "   - Defines where the workflow starts\n",
    "5. Compile the graph: `graph = graph_builder.compile()`\n",
    "\n",
    "### Graph Construction:\n",
    "- **StateGraph(State)**: Initialize a graph that manages our State schema\n",
    "- **add_sequence([retrieve, generate])**: Chain the functions sequentially\n",
    "- **add_edge(START, \"retrieve\")**: Define the entry point\n",
    "- **compile()**: Build the executable graph\n",
    "\n",
    "### Benefits of LangGraph:\n",
    "- **State Persistence**: Automatically passes state between nodes\n",
    "- **Debugging**: Can visualize and trace the workflow\n",
    "- **Flexibility**: Easy to add steps (e.g., query rewriting, re-ranking)\n",
    "- **Streaming**: Can stream intermediate results\n",
    "\n",
    "**Expected Output:** No output, but the compiled graph is ready to process questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37bda5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import START and StateGraph from langgraph.graph\n",
    "\n",
    "\n",
    "# TODO: Create a StateGraph instance with the State schema\n",
    "\n",
    "\n",
    "# TODO: Add the retrieve and generate functions as a sequence\n",
    "\n",
    "\n",
    "# TODO: Add an edge from START to retrieve\n",
    "\n",
    "\n",
    "# TODO: Compile the graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc4463",
   "metadata": {},
   "source": [
    "## Step 12: Visualize the Workflow Graph\n",
    "\n",
    "**Visual Representation** of our RAG pipeline using Mermaid diagrams.\n",
    "\n",
    "### Your Task:\n",
    "Generate and display a visual diagram of the workflow.\n",
    "\n",
    "**Steps:**\n",
    "1. Import display tools:\n",
    "   - `Image` and `display` from `IPython.display`\n",
    "2. Get the graph visualization: `graph.get_graph().draw_mermaid_png()`\n",
    "3. Display it: `display(Image(graph.get_graph().draw_mermaid_png()))`\n",
    "\n",
    "### What to Expect:\n",
    "The diagram will show:\n",
    "- **Nodes**: Each step in the workflow (`retrieve`, `generate`)\n",
    "- **Edges**: The flow of data between steps\n",
    "- **Entry Point**: Where the workflow starts (START → retrieve)\n",
    "- **End Point**: Where it completes\n",
    "\n",
    "This visualization helps understand the execution flow and is useful for:\n",
    "- Debugging complex workflows\n",
    "- Documentation and team communication\n",
    "- Identifying optimization opportunities\n",
    "\n",
    "**Expected Output:** A visual diagram showing the RAG pipeline flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8787a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import Image and display from IPython.display\n",
    "\n",
    "\n",
    "# TODO: Display the graph visualization using graph.get_graph().draw_mermaid_png()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd8521",
   "metadata": {},
   "source": [
    "## Step 13: Test the RAG System\n",
    "\n",
    "**First Query** - Testing the complete pipeline with a sample question.\n",
    "\n",
    "### Your Task:\n",
    "Invoke the RAG graph with a question and examine the results.\n",
    "\n",
    "**Steps:**\n",
    "1. Invoke the graph with a question: `result = graph.invoke({\"question\": \"What is Task Decomposition?\"})`\n",
    "2. Print the context: `print(f\"Context: {result['context']}\\\\n\\\\n\")`\n",
    "3. Print the answer: `print(f\"Answer: {result['answer']}\")`\n",
    "\n",
    "### Execution Flow:\n",
    "1. **Input**: `{\"question\": \"What is Task Decomposition?\"}`\n",
    "2. **Retrieve Step**: \n",
    "   - Converts question to embedding\n",
    "   - Finds most similar document chunks from the blog post\n",
    "   - Returns relevant context about task decomposition\n",
    "3. **Generate Step**:\n",
    "   - Formats prompt with question + retrieved context\n",
    "   - LLM generates answer based on the context\n",
    "4. **Output**: Complete result with context and answer\n",
    "\n",
    "### Expected Result:\n",
    "- **Context**: The actual document chunks retrieved from the vector store\n",
    "- **Answer**: A concise explanation of Task Decomposition based on the blog content\n",
    "\n",
    "**This is RAG in action! The LLM is answering based on retrieved context, not just its training data.**\n",
    "\n",
    "**Expected Output:** Retrieved context documents and a generated answer about task decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6285f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Invoke the graph with the question \"What is Task Decomposition?\"\n",
    "\n",
    "\n",
    "# TODO: Print the context from the result\n",
    "\n",
    "\n",
    "# TODO: Print the answer from the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4432286",
   "metadata": {},
   "source": [
    "## Step 14: Stream the Workflow Updates\n",
    "\n",
    "**Streaming Mode: Updates** - Observe each step of the workflow as it executes.\n",
    "\n",
    "### Your Task:\n",
    "Stream the workflow execution to see state changes at each step.\n",
    "\n",
    "**Steps:**\n",
    "1. Use `graph.stream()` with `stream_mode=\"updates\"`:\n",
    "   ```python\n",
    "   for step in graph.stream({\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"):\n",
    "       print(f\"{step}\\n\\n----------------\\n\")\n",
    "   ```\n",
    "\n",
    "### What is `stream_mode=\"updates\"`?\n",
    "- Shows the state changes after each node completes\n",
    "- Provides visibility into the pipeline execution\n",
    "- Useful for debugging and understanding the workflow\n",
    "\n",
    "### Output Format:\n",
    "You'll see two updates:\n",
    "1. **After `retrieve`**: Shows the retrieved context documents\n",
    "2. **After `generate`**: Shows the generated answer\n",
    "\n",
    "This is helpful for:\n",
    "- **Debugging**: Identify which step is slow or failing\n",
    "- **Validation**: Verify the right documents are being retrieved\n",
    "- **Monitoring**: Track progress in real-time applications\n",
    "\n",
    "**Expected Output:** Two dictionaries showing state updates after retrieve and generate steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c8ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Stream the workflow with stream_mode=\"updates\" and print each step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b6f14",
   "metadata": {},
   "source": [
    "## Step 15: Stream LLM Messages (Token-by-Token)\n",
    "\n",
    "**Streaming Mode: Messages** - Watch the LLM generate the response in real-time.\n",
    "\n",
    "### Your Task:\n",
    "Stream the LLM's response as it's generated, token by token.\n",
    "\n",
    "**Steps:**\n",
    "1. Use `graph.stream()` with `stream_mode=\"messages\"`:\n",
    "   ```python\n",
    "   for message, metadata in graph.stream({\"question\": \"What is Task Decomposition?\"}, stream_mode=\"messages\"):\n",
    "       print(message.content, end=\"|\")\n",
    "   ```\n",
    "\n",
    "### What is `stream_mode=\"messages\"`?\n",
    "- Streams individual messages/tokens as they're generated by the LLM\n",
    "- Shows the answer being constructed incrementally\n",
    "- Similar to how ChatGPT displays responses word-by-word\n",
    "\n",
    "### Use Cases:\n",
    "- **User Experience**: Display progressive responses in chat interfaces\n",
    "- **Real-time Feedback**: Users see that processing is happening\n",
    "- **Early Termination**: Can stop generation if the answer is sufficient\n",
    "\n",
    "### Output:\n",
    "- Each token/chunk of the LLM response is printed as it's generated\n",
    "- The `|` separator shows where each chunk ends\n",
    "- This demonstrates the streaming capability for production applications\n",
    "\n",
    "**Expected Output:** The answer text appearing gradually, separated by `|` characters\n",
    "\n",
    "**This completes your RAG implementation! 🎉**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875bb809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Stream the workflow with stream_mode=\"messages\" and print each message token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e1aaf3",
   "metadata": {},
   "source": [
    "## Congratulations! 🎉\n",
    "\n",
    "You've successfully built a complete RAG (Retrieval-Augmented Generation) system using LangChain, Azure OpenAI, and LangGraph!\n",
    "\n",
    "### What You've Accomplished:\n",
    "- ✅ Set up secure API key management\n",
    "- ✅ Configured Azure OpenAI embeddings for semantic search\n",
    "- ✅ Initialized a language model for text generation\n",
    "- ✅ Created a vector store for efficient similarity search\n",
    "- ✅ Loaded and parsed web content\n",
    "- ✅ Split documents into optimal chunks\n",
    "- ✅ Indexed documents with embeddings\n",
    "- ✅ Loaded and understood RAG prompt templates\n",
    "- ✅ Defined state management for workflow\n",
    "- ✅ Built a complete RAG pipeline with LangGraph\n",
    "- ✅ Visualized the workflow\n",
    "- ✅ Tested the system with queries\n",
    "- ✅ Implemented streaming for real-time responses\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "1. **RAG Architecture**: Combining retrieval and generation for grounded responses\n",
    "2. **Vector Embeddings**: Converting text to numerical representations\n",
    "3. **Semantic Search**: Finding relevant information by meaning, not keywords\n",
    "4. **State Management**: Managing data flow through complex workflows\n",
    "5. **LangGraph**: Building stateful, multi-step AI applications\n",
    "6. **Streaming**: Providing real-time user feedback\n",
    "\n",
    "### Next Steps:\n",
    "- Try different questions about AI agents\n",
    "- Experiment with different chunk sizes and retrieval parameters\n",
    "- Add more documents to the knowledge base\n",
    "- Implement query rewriting or re-ranking for better retrieval\n",
    "- Deploy this as a web API or chatbot\n",
    "\n",
    "### Challenge Exercises:\n",
    "1. Modify the system to load multiple web pages\n",
    "2. Add a step to the workflow that validates if retrieved context is relevant\n",
    "3. Implement a fallback mechanism when no good context is found\n",
    "4. Create a custom prompt template for a specific domain (e.g., technical support)\n",
    "5. Add conversation memory to make the system handle follow-up questions\n",
    "\n",
    "**You now have a production-ready foundation for building intelligent question-answering systems!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
