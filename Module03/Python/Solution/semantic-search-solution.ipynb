{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1227fb28",
   "metadata": {},
   "source": [
    "# Semantic Search with LangChain and Azure OpenAI\n",
    "\n",
    "This notebook demonstrates how to build a semantic search system using vector embeddings. Unlike traditional keyword-based search, semantic search understands the **meaning** of queries and documents, enabling more intelligent and context-aware information retrieval.\n",
    "\n",
    "## What is Semantic Search?\n",
    "\n",
    "Semantic search uses machine learning models to convert text into numerical vectors (embeddings) that capture semantic meaning. Documents with similar meanings will have similar vector representations, allowing us to find relevant information based on conceptual similarity rather than just keyword matching.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Load Documents**: Import PDF documents into processable format\n",
    "2. **Split Text**: Divide large documents into smaller, focused chunks\n",
    "3. **Generate Embeddings**: Convert text chunks into vector representations\n",
    "4. **Store Vectors**: Index embeddings in a vector database\n",
    "5. **Search**: Query the system to find semantically similar content\n",
    "\n",
    "Let's dive into each step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bfaf40",
   "metadata": {},
   "source": [
    "## Step 1: Loading Documents\n",
    "\n",
    "**Document loading** is the foundation of any information retrieval system. We'll load a PDF file containing Nike's 10-K financial report from 2023.\n",
    "\n",
    "### Why PDFs?\n",
    "- Common format for reports, papers, and documentation\n",
    "- Contains structured and unstructured data\n",
    "- Requires specialized parsing to extract text accurately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933a6ce2",
   "metadata": {},
   "source": [
    "### Import PDF Loader and Load Document\n",
    "\n",
    "**PyPDFLoader** is a specialized document loader for PDF files in LangChain.\n",
    "\n",
    "### How it works:\n",
    "- **Reads PDF**: Parses the binary PDF format\n",
    "- **Extracts Text**: Pulls text content from each page\n",
    "- **Creates Documents**: Converts each page into a LangChain `Document` object with:\n",
    "  - `page_content`: The actual text from the page\n",
    "  - `metadata`: Information like file path and page number\n",
    "\n",
    "### The Process:\n",
    "1. Specify the file path to the PDF\n",
    "2. Create a loader instance\n",
    "3. Call `load()` to extract all pages\n",
    "\n",
    "**Result**: A list of Document objects, one per PDF page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b2c059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"../../data/nke-10k-2023.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d49fd",
   "metadata": {},
   "source": [
    "### Understanding Document Structure\n",
    "\n",
    "`PyPDFLoader` creates one `Document` object per PDF page, making it easy to work with the content programmatically.\n",
    "\n",
    "### Each Document contains:\n",
    "- **`page_content`**: The string content of the page (all text extracted)\n",
    "- **`metadata`**: A dictionary with:\n",
    "  - `source`: File path to the original PDF\n",
    "  - `page`: Page number (0-indexed)\n",
    "\n",
    "This structure allows us to:\n",
    "- Track where information came from\n",
    "- Reference specific pages when displaying results\n",
    "- Filter or process specific sections of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41700b68",
   "metadata": {},
   "source": [
    "### Inspect Document Content and Metadata\n",
    "\n",
    "Let's examine what was extracted from the first page to verify the loading process worked correctly.\n",
    "\n",
    "**This preview shows:**\n",
    "- First 200 characters of the page content\n",
    "- Metadata dictionary with source file and page number\n",
    "\n",
    "This verification step helps ensure:\n",
    "- Text extraction is working properly\n",
    "- Content quality is sufficient for semantic search\n",
    "- Metadata is correctly populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96db459c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table of Contents\n",
      "UNITED STATES\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "Washington, D.C. 20549\n",
      "FORM 10-K\n",
      "(Mark One)\n",
      "☑  ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "F\n",
      "\n",
      "{'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'creator': 'EDGAR Filing HTML Converter', 'creationdate': '2023-07-20T16:22:00-04:00', 'title': '0000320187-23-000039', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'keywords': '0000320187-23-000039; ; 10-K', 'moddate': '2023-07-20T16:22:08-04:00', 'source': '../../data/nke-10k-2023.pdf', 'total_pages': 107, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{docs[0].page_content[:200]}\\n\")\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183953d7",
   "metadata": {},
   "source": [
    "## Step 2: Document Splitting Strategy\n",
    "\n",
    "### Why Split Documents?\n",
    "\n",
    "**Problem**: A full PDF page is often too large and contains multiple topics.\n",
    "- Mixing different concepts in one chunk dilutes the semantic meaning\n",
    "- Large chunks make it harder to pinpoint specific information\n",
    "- Retrieval accuracy suffers when relevant details are buried in irrelevant context\n",
    "\n",
    "**Solution**: Split documents into smaller, focused chunks.\n",
    "\n",
    "### Text Splitter Configuration\n",
    "\n",
    "We use `RecursiveCharacterTextSplitter` with these parameters:\n",
    "- **`chunk_size=1000`**: Target 1000 characters per chunk (approximately 150-200 words)\n",
    "- **`chunk_overlap=200`**: Overlap consecutive chunks by 200 characters\n",
    "  - Prevents splitting related sentences across chunks\n",
    "  - Maintains context continuity at chunk boundaries\n",
    "- **`add_start_index=True`**: Tracks original position in source document\n",
    "\n",
    "### Why \"Recursive\"?\n",
    "\n",
    "The splitter tries to split on natural boundaries in this order:\n",
    "1. Paragraphs (double newlines)\n",
    "2. Sentences (single newlines)\n",
    "3. Words (spaces)\n",
    "4. Characters (as last resort)\n",
    "\n",
    "This preserves semantic coherence better than arbitrary character splitting.\n",
    "\n",
    "**Learn more**: [LangChain PDF Guide](/docs/how_to/document_loader_pdf/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c32e7",
   "metadata": {},
   "source": [
    "### Split Documents into Manageable Chunks\n",
    "\n",
    "**Execute the splitting** to transform the page-level documents into smaller, focused segments.\n",
    "\n",
    "### What happens:\n",
    "1. Each page document is processed by the text splitter\n",
    "2. Pages are divided into ~1000 character chunks\n",
    "3. Chunks maintain 200 character overlap with neighbors\n",
    "4. Original document metadata is preserved in each chunk\n",
    "5. Start index is added to track position in original document\n",
    "\n",
    "### Expected Result:\n",
    "The number of chunks will be significantly larger than the number of pages, as each page typically produces multiple chunks.\n",
    "\n",
    "**Benefits for Search:**\n",
    "- More precise retrieval (find specific paragraphs, not whole pages)\n",
    "- Better embedding quality (focused semantic meaning)\n",
    "- Improved answer accuracy in downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a88ac92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'creator': 'EDGAR Filing HTML Converter', 'creationdate': '2023-07-20T16:22:00-04:00', 'title': '0000320187-23-000039', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'keywords': '0000320187-23-000039; ; 10-K', 'moddate': '2023-07-20T16:22:08-04:00', 'source': '../../data/nke-10k-2023.pdf', 'total_pages': 107, 'page': 0, 'page_label': '1', 'start_index': 0}, page_content=\"Table of Contents\\nUNITED STATES\\nSECURITIES AND EXCHANGE COMMISSION\\nWashington, D.C. 20549\\nFORM 10-K\\n(Mark One)\\n☑  ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\\nFOR THE FISCAL YEAR ENDED MAY 31, 2023\\nOR\\n☐  TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\\nFOR THE TRANSITION PERIOD FROM                         TO                         .\\nCommission File No. 1-10635\\nNIKE, Inc.\\n(Exact name of Registrant as specified in its charter)\\nOregon 93-0584541\\n(State or other jurisdiction of incorporation) (IRS Employer Identification No.)\\nOne Bowerman Drive, Beaverton, Oregon 97005-6453\\n(Address of principal executive offices and zip code)\\n(503) 671-6453\\n(Registrant's telephone number, including area code)\\nSECURITIES REGISTERED PURSUANT TO SECTION 12(B) OF THE ACT:\\nClass B Common Stock NKE New York Stock Exchange\\n(Title of each class) (Trading symbol) (Name of each exchange on which registered)\")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94a8552",
   "metadata": {},
   "source": [
    "## Step 3: Configure Azure OpenAI Embeddings\n",
    "\n",
    "**Embeddings** are the heart of semantic search - they convert text into numerical vectors that capture meaning.\n",
    "\n",
    "### What are Embeddings?\n",
    "- **Input**: Text string (query or document)\n",
    "- **Output**: Vector of numbers (e.g., 1536 dimensions for text-embedding-ada-002)\n",
    "- **Property**: Similar meanings → Similar vectors\n",
    "\n",
    "### Model: text-embedding-ada-002\n",
    "- **Provider**: Azure OpenAI\n",
    "- **Dimensions**: 1536-dimensional vectors\n",
    "- **Use Cases**: Semantic search, clustering, similarity comparison\n",
    "- **Quality**: Captures nuanced semantic relationships\n",
    "\n",
    "### Configuration:\n",
    "- **`azure_endpoint`**: Your Azure OpenAI resource URL\n",
    "- **`api_key`**: Authentication credential (loaded securely)\n",
    "- **`model`**: The deployment name of the embedding model\n",
    "- **`api_version`**: Azure API version for compatibility\n",
    "\n",
    "### Security Note:\n",
    "The code checks for an existing environment variable before prompting, supporting both:\n",
    "- Local development (with manual input)\n",
    "- Production environments (with pre-configured env vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ca71060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=\"https://aoi-ext-eus-aiml-profx-01.openai.azure.com/\",\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    api_version=\"2024-12-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b440635",
   "metadata": {},
   "source": [
    "### Generate and Inspect Sample Embeddings\n",
    "\n",
    "**Test the embeddings model** by converting two document chunks into vectors and examining the results.\n",
    "\n",
    "### What this demonstrates:\n",
    "1. **`embed_query()`**: Converts text to a vector embedding\n",
    "2. **Consistency**: Both embeddings have the same dimensionality\n",
    "3. **Vector Format**: Shows the actual numerical values\n",
    "\n",
    "### Understanding the Output:\n",
    "- **Vector Length**: Typically 1536 for text-embedding-ada-002\n",
    "- **Vector Values**: Floating-point numbers (usually between -1 and 1)\n",
    "- **First 10 Values**: Preview of the embedding (each dimension captures different semantic features)\n",
    "\n",
    "### Key Insight:\n",
    "Even though the text content is different, the embeddings are comparable (same length, same format). We can measure similarity between these vectors using mathematical operations like cosine similarity or Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d74e9c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated vectors of length 1536\n",
      "\n",
      "[-0.00860656425356865, -0.03344116732478142, -0.009941618889570236, -0.0050745029002428055, 0.009079665876924992, 0.009442593902349472, -0.028230568394064903, -0.01646135002374649, 0.002953645773231983, -0.012832076288759708]\n"
     ]
    }
   ],
   "source": [
    "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
    "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(vector_1) == len(vector_2)\n",
    "print(f\"Generated vectors of length {len(vector_1)}\\n\")\n",
    "print(vector_1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb3c277",
   "metadata": {},
   "source": [
    "## Step 4: Initialize the Vector Store\n",
    "\n",
    "**Vector Stores** (or Vector Databases) are specialized databases optimized for storing and searching high-dimensional vectors.\n",
    "\n",
    "### What is InMemoryVectorStore?\n",
    "- **Type**: In-memory storage (not persisted to disk)\n",
    "- **Speed**: Fast for development and prototyping\n",
    "- **Scope**: Data exists only during the session\n",
    "- **Best For**: Testing, small datasets, demonstrations\n",
    "\n",
    "### How it Works:\n",
    "- Stores document text alongside their embedding vectors\n",
    "- Enables fast similarity searches using vector math\n",
    "- Automatically uses the embeddings model we configured\n",
    "\n",
    "### Production Alternatives:\n",
    "For production use with large datasets or persistence requirements, consider:\n",
    "- **Chroma**: Local, persistent vector database\n",
    "- **Pinecone**: Managed cloud vector database\n",
    "- **Azure AI Search**: Azure's vector search service\n",
    "- **Weaviate**: Open-source vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1f02c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639608e",
   "metadata": {},
   "source": [
    "## Step 5: Index Documents in the Vector Store\n",
    "\n",
    "**Indexing** is the process of storing documents with their embeddings for later retrieval.\n",
    "\n",
    "### What happens in `add_documents()`:\n",
    "1. **For each document chunk**:\n",
    "   - Send the text to the embeddings model\n",
    "   - Receive back a 1536-dimensional vector\n",
    "   - Store both the text and vector in the database\n",
    "   - Generate a unique ID for tracking\n",
    "\n",
    "2. **Build the index**:\n",
    "   - Organize vectors for efficient similarity search\n",
    "   - Create data structures for fast nearest-neighbor lookup\n",
    "\n",
    "### Performance Note:\n",
    "This operation makes API calls to Azure OpenAI for each chunk, so it may take some time depending on:\n",
    "- Number of chunks\n",
    "- API rate limits\n",
    "- Network latency\n",
    "\n",
    "### Output:\n",
    "Returns a list of unique IDs for each stored document, confirming successful indexing.\n",
    "\n",
    "**Your knowledge base is now ready for semantic search!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "038b65c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff8af7",
   "metadata": {},
   "source": [
    "## Step 6: Perform Your First Similarity Search\n",
    "\n",
    "**Semantic search in action!** Let's query the system to find information about Nike's distribution centers.\n",
    "\n",
    "### How Similarity Search Works:\n",
    "1. **Convert Query**: The question is converted to an embedding vector\n",
    "2. **Compare Vectors**: The system compares the query vector to all stored document vectors\n",
    "3. **Calculate Similarity**: Uses cosine similarity or distance metrics\n",
    "4. **Rank Results**: Returns the most similar documents\n",
    "5. **Default**: Returns top 4 most relevant chunks\n",
    "\n",
    "### Query: \"How many distribution centers does Nike have in the US?\"\n",
    "\n",
    "**Why this is powerful:**\n",
    "- No exact keyword matching required\n",
    "- Understands \"distribution centers\" relates to \"facilities,\" \"warehouses,\" etc.\n",
    "- Captures semantic intent of the question\n",
    "- Finds relevant content even with different wording\n",
    "\n",
    "### Expected Output:\n",
    "The most relevant document chunk containing information about Nike's distribution infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "503de265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\n",
      "U.S. RETAIL STORES NUMBER\n",
      "NIKE Brand factory stores 213 \n",
      "NIKE Brand in-line stores (including employee-only stores) 74 \n",
      "Converse stores (including factory stores) 82 \n",
      "TOTAL 369 \n",
      "In the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\n",
      "2023 FORM 10-K 2' metadata={'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'creator': 'EDGAR Filing HTML Converter', 'creationdate': '2023-07-20T16:22:00-04:00', 'title': '0000320187-23-000039', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'keywords': '0000320187-23-000039; ; 10-K', 'moddate': '2023-07-20T16:22:08-04:00', 'source': '../../data/nke-10k-2023.pdf', 'total_pages': 107, 'page': 4, 'page_label': '5', 'start_index': 3125}\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"How many distribution centers does Nike have in the US?\"\n",
    ")\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1b043",
   "metadata": {},
   "source": [
    "## Step 7: Async Similarity Search\n",
    "\n",
    "**Asynchronous search** enables non-blocking operations, crucial for production applications.\n",
    "\n",
    "### What is Async?\n",
    "- **Traditional (Sync)**: Code waits for search to complete before continuing\n",
    "- **Async**: Search runs in background, allowing other operations simultaneously\n",
    "- **Use Cases**: Web applications, API endpoints, concurrent queries\n",
    "\n",
    "### Benefits of Async Search:\n",
    "- **Responsiveness**: UI remains interactive during search\n",
    "- **Scalability**: Handle multiple search requests concurrently\n",
    "- **Performance**: Better resource utilization in I/O-bound operations\n",
    "\n",
    "### Query: \"When was Nike incorporated?\"\n",
    "\n",
    "**Note**: In Jupyter notebooks, async functions are called with `await` and work seamlessly. In regular Python scripts, you'd need to use `asyncio.run()` or an event loop.\n",
    "\n",
    "**Expected Output:**\n",
    "Document chunk containing information about Nike's founding/incorporation date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "272bdc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Table of Contents\n",
      "PART I\n",
      "ITEM 1. BUSINESS\n",
      "GENERAL\n",
      "NIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\n",
      "\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\n",
      "Our principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\n",
      "the largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\n",
      "and sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales' metadata={'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'creator': 'EDGAR Filing HTML Converter', 'creationdate': '2023-07-20T16:22:00-04:00', 'title': '0000320187-23-000039', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'keywords': '0000320187-23-000039; ; 10-K', 'moddate': '2023-07-20T16:22:08-04:00', 'source': '../../data/nke-10k-2023.pdf', 'total_pages': 107, 'page': 3, 'page_label': '4', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "results = await vector_store.asimilarity_search(\"When was Nike incorporated?\")\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd196d06",
   "metadata": {},
   "source": [
    "## Step 8: Similarity Search with Confidence Scores\n",
    "\n",
    "**Understanding search quality** by examining similarity scores alongside results.\n",
    "\n",
    "### What are Similarity Scores?\n",
    "- **Purpose**: Quantify how well each result matches the query\n",
    "- **Range**: Depends on the distance metric used\n",
    "- **Interpretation**: Lower scores = higher similarity (for distance metrics)\n",
    "\n",
    "### Distance Metrics:\n",
    "Different vector stores use different scoring methods:\n",
    "- **Cosine Distance**: Measures angle between vectors (0 = identical, 2 = opposite)\n",
    "- **Euclidean Distance**: Straight-line distance in vector space\n",
    "- **Dot Product**: Inner product of vectors\n",
    "\n",
    "### Why Scores Matter:\n",
    "- **Filtering**: Set thresholds to exclude low-quality matches\n",
    "- **Ranking**: Sort results by relevance\n",
    "- **Confidence**: Determine if results are trustworthy\n",
    "- **Debugging**: Identify when queries aren't matching well\n",
    "\n",
    "### Query: \"What was Nike's revenue in 2023?\"\n",
    "\n",
    "**Expected Output:**\n",
    "- The most relevant document chunk\n",
    "- A numerical score indicating match quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02f62c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8807336730352323\n",
      "\n",
      "page_content='Table of Contents\n",
      "FISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTSThe following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:\n",
      "FISCAL 2023 COMPARED TO FISCAL 2022\n",
      "• NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.\n",
      "The increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,\n",
      "2 and 1 percentage points to NIKE, Inc. Revenues, respectively.\n",
      "• NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This\n",
      "increase was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesale\n",
      "equivalent basis.' metadata={'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'creator': 'EDGAR Filing HTML Converter', 'creationdate': '2023-07-20T16:22:00-04:00', 'title': '0000320187-23-000039', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'keywords': '0000320187-23-000039; ; 10-K', 'moddate': '2023-07-20T16:22:08-04:00', 'source': '../../data/nke-10k-2023.pdf', 'total_pages': 107, 'page': 35, 'page_label': '36', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "# Note that providers implement different scores; the score here\n",
    "# is a distance metric that varies inversely with similarity.\n",
    "\n",
    "results = vector_store.similarity_search_with_score(\"What was Nike's revenue in 2023?\")\n",
    "doc, score = results[0]\n",
    "print(f\"Score: {score}\\n\")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a832d6f",
   "metadata": {},
   "source": [
    "## Step 9: Search with Pre-computed Embeddings\n",
    "\n",
    "**Advanced technique**: Search using vectors directly instead of text queries.\n",
    "\n",
    "### Why Use Pre-computed Embeddings?\n",
    "\n",
    "**Scenario 1 - Performance Optimization:**\n",
    "- Generate query embedding once\n",
    "- Reuse it for multiple searches\n",
    "- Reduces API calls and latency\n",
    "\n",
    "**Scenario 2 - Advanced Workflows:**\n",
    "- Search with modified/combined embeddings\n",
    "- Implement custom similarity logic\n",
    "- Build hybrid search systems\n",
    "\n",
    "**Scenario 3 - Cross-modal Search:**\n",
    "- Search documents using image embeddings\n",
    "- Find similar concepts across different data types\n",
    "\n",
    "### The Two-Step Process:\n",
    "1. **`embed_query()`**: Convert text to embedding vector\n",
    "2. **`similarity_search_by_vector()`**: Search using the vector directly\n",
    "\n",
    "### Query: \"How were Nike's margins impacted in 2023?\"\n",
    "\n",
    "**Use Case:**\n",
    "This pattern is especially useful when:\n",
    "- Building search APIs (cache embeddings)\n",
    "- Implementing recommendation systems\n",
    "- Creating multi-step search pipelines\n",
    "\n",
    "**Expected Output:**\n",
    "Same quality results as text-based search, but with more control over the embedding process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5fa8166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Table of Contents\n",
      "GROSS MARGIN\n",
      "FISCAL 2023 COMPARED TO FISCAL 2022\n",
      "For fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to\n",
      "43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:\n",
      "*Wholesale equivalent\n",
      "The decrease in gross margin for fiscal 2023 was primarily due to:\n",
      "• Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as\n",
      "product mix;\n",
      "• Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in\n",
      "the prior period resulting from lower available inventory supply;\n",
      "• Unfavorable changes in net foreign currency exchange rates, including hedges; and\n",
      "• Lower off-price margin, on a wholesale equivalent basis.\n",
      "This was partially offset by:' metadata={'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'creator': 'EDGAR Filing HTML Converter', 'creationdate': '2023-07-20T16:22:00-04:00', 'title': '0000320187-23-000039', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'keywords': '0000320187-23-000039; ; 10-K', 'moddate': '2023-07-20T16:22:08-04:00', 'source': '../../data/nke-10k-2023.pdf', 'total_pages': 107, 'page': 36, 'page_label': '37', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "embedding = embeddings.embed_query(\"How were Nike's margins impacted in 2023?\")\n",
    "\n",
    "results = vector_store.similarity_search_by_vector(embedding)\n",
    "print(results[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-module03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
